
1/12

    This is the Design-Build-Test suite for object tracking
        Build: create a function in ControlTracker
        Design: get into jupyter and look at indv. steps within the process. also, look at problem frames and try to adjust algo to make them work too.
        Test: deploy the built-tracker across a corpus of test cases
        overall: iterate on this process untill product req's are satisfied

1/8

how to step into debugger of vscode from jupyter call?
    attach external? to the ipy terminal or to chrome?
        "name": "Attach (Remote Debug)",
        "request": "attach",
        "port": 3000,
        "secret": "my_secret",
        "host": "localhost"
    ipy is on port localhost:8888 ; secret = "ipy token"
    log:
        [I 12:48:42.977 NotebookApp] Kernel started: 9bd7d855-a586-494f-aa2b-d8eb4fee0556
    or: we could pickle out of jupyter into the module's __main__ section

Here's how you could do it: 
    (possibly using an extension in the notebook for interproc comm...)
    create a tmp file in the same dir as the notebook, 
        debug_notebookname_tmp1.py
    collect all imports from the noteook, send those to temp file
        you'll have to eliminate "cell magics" and include sys.path changes
    encapsulate the cell to debug within a class/function? (this could be done
    with an extension's custom cell type)
    pickle that object to a tmp file debug_notebookname_obj1.pickle
    note you'll have to pickle custom package cpython types with their
        own command (or do you? will it just implement it automatically?)
    have the debug_notebookname_tmp1.py load the debug_notebookname_obj1
    execute the core within executable lines within via vscode-debug by launching
    the debugger on the tmp.py
    set a breakpoint before the program executes the first line
    voila?
            example: do it with tensorflow

    10 commands to ppd-objTracking
            e.g. 
                1 >python guiview.py --file --tracker --showscoring
                2 >python guiview.py --eval
                3 >load that table in jupyter
                4 >get listGS of failures
                5 >run analysis on fail-frames
                7 >guirecord.py 
                8 >timelog.py
                9
                10 >add the video to the data directory
                11 >run eval with new vid

                12 >write a new tracker
                13 >eval it

1/2

ai - cascading business-logic across a cognitive processing loop
    e.g. this project, skydio/autonomy in general, nlp like sentiment analysis
    define cognitive - arrange functions which process a datastream by use case on a spectrum of cognitive vs managerial; cognitive is input data (media) which contains deeper semantics, managerial data only has shallow meaning
    define cascading - a clever tool for enumerating all possible states policies from a limited set of first pricicples and product requirements

12/28

"virutal frames" 1MHz Framerate from iPhoneX but only for pure black&white scenes
https://www.technologyreview.com/s/612410/how-to-mod-a-smartphone-camera-so-it-shoots-a-million-frames-per-second/

for pt2. - config + positioning
https://stackoverflow.com/questions/33025904/ar-with-external-tracking-alignment-is-wrong-values-are-right

12/20

objTracker doesn't have to be perfect right now; in the next stages we'll add scope-limiting assumptions, knowledge priors, and agent policy restrictions. Here we just have to produce a decent baseline. Then we can see what we're still missing, even after the business logic fills-in some misses.

Most of the requirements in path construction/projection is actually a motion ray obj-tracking, so that will be the area to concentrate in the future.

But the config + positioning steps will use stationary tracking so this is helpful.

Comprehensive Evaluation - "eval module" - is next step.

12/19

Now that orange ball is tracked pretty succesfully, this initial phase should be tied off. From here, we have a baseline algorithm and can find some challenge frames within our existing videos. Todos still include:
 - challenge data
 - guiview --filter
 - eval module
 - experiments with dif off-the-shelf obj trackers
 - workspace tidying 

note: the logitech camera's dynamic focusing is interfering with tracking (oct20/output8)

12/17

It's interesting to plot and segment individual raw pixels, but it's not productive to our ultimate task to segment multi-pixel objects. Instead we should be combining averages of proximate pixels and drawing segmentation rules there.

But how does conflict with the far-away / low-pixel object objective? Because multi-pixels ("blurred"/aggregated) have the same plotting ability with higher signal to noise.

This generation of track-algo's without pixel aggregating is the extreme form of long distance find where even 1 good pixel can get us a find and we're looking for regions of them, but actually the span of tracking-algo's can be thought of as a spectrum of increasingly high-aggregate, increasingly low variation, and increasingly low noise. From there it will be easier to generalize color-space segmentation based on pre-video and in-video calibration. And a long distance find is a low-aggregation amount thus noisier, which we ideally don't want to do, but can if we conceive of this portion of the track-algo (candidate object identification) less as a hard classifier and more as a scoring, with a ranking.


12/16

Software Engineering advice is like nutritional advice; there's more than enough out there, most of it will help you, but you'd be better served by simply being disciplined and applying some crude heuristic and following it than in accumulating extensive knowledge. Second, there are actively harmful pieces of advice. Although these exist for a reason (b/c this does help certain people), it's not right for you, and you need to be able to be sensitive to this possibility and willing to pivot. But third, at the highest level, you do need to accumulate cutting edge knowledged and be willing to expand past the crude-discipline-paradigm.


11/25

random thought:
i'm interested in Diagnostic Information Engineering - not data engineering where overall volume is the goal - but figuring out what to look at and how to organize your workflow around key metrics and views

11/22

random thought on NN's:
NN's perfrom well in domains - like NLP, sound processing, img processing - where the concept of a probability distribution function is not realistic (too much dimensionality, no good prior, etc...). Chomsky has written on this in terms of language with "orange dreams furiously" as a point on a pdf or the "what is happening outside the physics dept's window" thought experiment. I beleive there is no first priciples way to build a confidence interval from scoring on an NN model, only to build a true holdout dataset and evaluate the trained model against it. In contrast classic stats has CI fromulas galroe, e.g Bufferoni CI for regression parameters. Since classical statistics is obsessed with pdf's, it's not surprising that domains that "lacked" this feature (or where it was inappropriately shoe-horned) and much more perfrormant when discarding it.

11/21

We don't want a "good" object tracker, we want a configureable/adjustable object tracker.

think about when TV was first invented and there was no high-thruput
digital data lines: sure a tv in perfect unobstructed line of sight to a broadcast tower would work, but what about at ground level of Times Square? And then everything in between: theres a huge space of possible
interference that we can't build into the logic of the signal-receiver.
instead we could alleviate individual situations with trial and error by adding more distortion in the proximity, using our body as antenna. But a better solution is the rabbit ears, a simple machine, but highly configurable for each situation to make it work, and allows each individual situation to work.

So what we're looking for is computer vision "rabbit ears", a powerful yet simple configuration module. That's what will let this pretty straightforward technology work and be mass-useable outside the lab.

11/7

guiview is essentially the skeleton of a training-data management system. it references frame-data, time-data, note-data, interfaces with track-calcs, so this is how we'd build a total dataset evaluation. it's not just a custom photoshop-like program for hand edits, but a template for app development.

the thorniness of the problem is that
first, the a tracker at frametime = t, depends on data from input-data from time in [0, t) in a complex hierarchy, e.g.
    what was the position last frame?
    which direction was it moving last frame?
    what is the range of threshholds set during calibration
    what does the background mask look like?
    what does the agent mask look like from beginning of vid?
and there's no automated way to specify these features during record, only with hand processing. So frames depend on the frames before them so we can't consider them as independant records. [*0]

second, that while each video is independant, there are differing variables like ball-color, frame-size, frame-speed, lighting-level, that vary between them.

third, that the parameters for the tracking algo affect the calibration phase which then affects the evaluation stage, so updating the tracker at frame_t requires first updating previous frames with the parameters.

basically: we're trying to find a way to build a bunch of videos with a calibration phase that then do well* in the evaluation. *well meaning accurate and fast, and the cost of accuracy is very subtle because say we can afford to miss one frame within a bounce if we reliably track the two adjacent frames.

another aspect is that we don't have a ground-truth, it's costly to build it by hand. so we need an iterative process to do that: find the false-negatives visually, score it by hand, then save it for future automatic evaluation.

one other thing of note: once trackers get good enough, they can be used construct pseudo-ground-truths, and that automates construction of the ground truth, our dataset bootstraps! Once there's a large availability of training data (diversity of frames and diversity of conditions), then that can be used to train a potentially more accurate blackbox. but starting out we can only build models by hand using intuition, and trial and error using visual evaluation.

[*0]: but this is what we want; we want to utilize the limititation assumptions of a cooperative agent - "the human player" - with a defined task - throwing a ball at a vertain vector range - and use those to optimize the accuracy and speed of the algo.
opencv-contrib object detectors are superior for the general case of object detection (and general case of liberal assumptions) no doubt, but we're writing a workspace that finds how much we can accuracy/perf we can squeeze out of those assumptions, and still work in a variety of situations.

another note: I think the bounce will really be a much different algo, maybe even easier in some ways b/c it will ruthlessly look for ray-type diffs but the semi-static object tracker is neccesary for drone positioning.

10/29

Now can account for multiple tracking objects thoughout workflow: this will help with tracking reflection object in future training videos. Need to quickly close out wokr in guiview and move onto: building training data, refining tracker, batch evaluation. Final big steps are unit testing for guiview and guirecord.

10/5

lots of little bugs seem still present in guiview; mostly around the build/resize of zoom frame and how that changes each frame when tracking / scoring are providing the coords for thie display.
need to divorce the zoom_display from tracking and scoring displays in order to interactively
build experiment videos. Need to get to a real example of building an experiment video.

10/2

guiview is ready for action in processing training videos.
Now we need to build in the tracking algo. 
How to do an "online"/"live" tracking server and/or an already processed tracking result?
How could I use this with Ipython? 
    first, you could send the current image directly into Ipython while running with gui,
    or you could query a batch from the main loop, into a list of frames and associated data

10/9

Locally, both spatially, and within frame, we can look for the ball contour, and expand the ROI to the full ball. Then we can grab the full range of color contained within.

We can do a calibration stage where ball is put everywhere in the room, and then we'll have a (3D) segmented map of expected color.

Note that reflection will have different color of ball b/c it's getting light from a different angle, the underside of the ball.


9/28

Finish up main problems with Display,
Then write getScore -> notesFactory (and gui element)
    [ ] do a simple example, where you mark a few then that shows on replay
Then do some unit tests on Display orig<->main<->zoom coords (w/w/o modulo-0)

9/25

OpenCV comes with lots of good demos in samples/, e.g. color_histogram.py is much faster than I could acheive; I should investigate these "samples" in the context of my own data.

Also, were going to investigate pyimagesearch tutorial on tracking and apply it to the data as a null.

Note: there's plans to 1.) package guirecord/guiview as python package 2.) make a compelling demo with chess piece clipping -> training data.

9/24

The major question guirecord was built around is how much framedelay occurs with each option of the program, and how the image may be altered due to compression. So it include timelog in its output analysis steps.

Thisn morning, I'm searching for a way to debug tkinter by reproducing the problem in a minimal app.

9/23

An experiment can be one video with particular frames:
    e.g. output4.proc1.avi
    first four frames are for training so I can save them as frame.purpose = training
    also save their frame num from original,
    and also save their 
    now some later frames are for scoring, so put writevid to ON and play,
        (if you know ball is present in all frames you can set that )
    these can help you see how tracking is performing.
    every once in a while, you pause and can select an ROI for the ball
    and set handscore = true, handscoreroi = (200,291)


9/20

Compare guiview + module.ControlFlow to guirecord:
    view the benefits of object oriented programming
     - isolation of concerns with counters
     - minimal indentation for control flow
     - handle validation and transforms outside of main
     - easier to test with stubs

How would I test gui_view? 
    run from subproc and output to stderr? 
    add stubs under b_test: send frame/time/directory Factory into TestData()
    then TestData.test1(),test2(),... etc calls queries on these objects
    also need mocks for test1() test2() so that playOn can be turned on etc.
    these stubs/mocks need to be setup at both positions in the loop

    this helps make sure --dir and --file function correctly as dev continues


9/19

We've invested time to write guiview to hand process training videos into precise tests.
This came after the workflow for adaptive_thresh.py became impossible to keep doing "live tests" of agenda. Now we can compare multiple training scenarios and multiple scoring tests and perf; and do that in multiple scenario, etc

guiview --dir --firstN / timelog.py  is unix like: "everything is a file" for videos; need to be able to manipulate with care and ease and review with power and flexibility.

So, we like cli for videos, but we also need interactivity - for record, to set up the frame, and for view, to hand score - so we need a gui, not just cli.

We can talk about mock testing for a gui: how you don't need interactivity to run checks
And how this helps us create tests, which allows us to switch opencv and ffmpeg versions


9/16

Looks like we're one frame behind the current frame within the loop as demonstated by the --sleepschedule experiments. This can be overcome with --funkyread but that cuts FPS in half. Instead, we just note we're one frame behind and offset frametimelog ahead one frame. 

We've looked at startInterval and compared it to frametimelog: when processing like writing vid is high, it forces startInterval down to zero, so this function seems to be compensating for too-fast loops by idling.

    To do:
    [x] verify this behavior is duplicated by logitech cam
    [x] verify buffer size is 1 with --funkyread

    Questions:
    [ ] why is startInterval 0 in cases where sleepschedule is applied?
    [ ] why doesn't grab/retrieve work as expected?
    [ ] why does opening a VideoCapture object provide a grabbed  frame to be retrieved?
        can this be alleviated?

9/15

even a .001 float drift per frame -> 1.0 seconds after 1,000 frames!

"just assume the frame was grabbed at the end of last frames read()

Need to do 2 days of this:
https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/


We're finding that framelog times dont correspond to the actual time between frames: it's only the next frame that has the delay. Check out grab/retrieve vs read() on videocapture class:
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html
https://docs.opencv.org/3.2.0/d8/dfe/classcv_1_1VideoCapture.html

This guys sees old frames in buffers, needs to path opencv directly
https://01.org/developerjourney/problems-camera-lag

    Here's the source for patches:
    https://github.com/jketreno/robot/tree/master/patches

People familiar with the problem
http://answers.opencv.org/question/29957/highguivideocapture-buffer-introducing-lag/

He says call grab five times
https://stackoverflow.com/questions/41412057/get-most-recent-frame-from-webcam

    it appears grab 5x or 11x doesn't work
    but retrieve 5x does work


It appears as soon as read() is called on 4th frame, another image is grabbed on camera
so when we go to 5th frame, after sleep we retrieve that image so it doesn't appear delayed.

Looking at frametime plots in analyze-frame-time.ipynb:
    cam.read() is deliberately slowing down the call time as it can go to 1 millisec 
    when the inter-frame-processing overhead forces it to.
    
    That's roughly 2/3 of processing time be used for dilly-dallying, if you needed more proc time, seek to put this on another thread or something.
    
It's still not clear if long start-end- intervals are reading camera's CCD at start or end of that time. But working hypothesis is it's at the end of the interval. Therefore, frametime is always accurate: when it's unusually long, cam just pulled that image.
When it's unusually short, cam pulled that image quickly.

n:       232
mean:    0.0334
min:     0.0250   3.3 sd
max:     0.0480   5.8 sd
sd:      0.0025

So -40% to +40% frame times; high is 100% larger than low
    -> which is significant for bounce projection

But what do they mean? What is the exact information that can be extracted?
What is the order of frametimelogs to the actual picture?

    To do:
    [x] build a deliberate time.sleep() on some frame and compare log to actual frames
    [x] build a frame_start_time log as well; interpret with timelog
    [ ] build a book showing a big frame jump and how that corresponds in frametime


9/14

Found issues with slower than 30 FPS when 1920 and recordOn = True. Multiple options have been added to gui to deal with this:
    codec       - prompt allows
    buffer      - on save frames to memory and only writes to disk 
                  at end-of-record / out of memory
    log type -   simple log (1 col of framerate data) only outputs saved frames data;
                  detailed log (multi col) outputs frames before

    To do:
    [x] add no-loss option to gui


    Notes:
     - can use detailed log, with timelog.py --filtervar recordOn --filterval True
       to debug slow fps
    - if, simple-log with timelog.py --skip 1 --hz is off


Explored compression and loss in books/compare-compression-frames.ipynb:
    png is lossless
    jpg is different encoding from h264
    diff is ~ 25% of pixels ?

    -> we now have strategies for dealing with compression alterations

    Todo:
    [ ] What is time to process each size image?


9/13

guirecord.py is pretty well positioned for it's use. As of yesterday we established frame rate is constant across different different gui options and framesizes. Also record does not delay framerate on laptop cam. (Still to try: logitech and picam)
    
    To remember:
    - use timelog.py to check summary stats on fps data
    - use b_timelog, b_log_vars in guirecord to affect logging behavior

    To do:
    [ ] apply guirecord to logitech and picam (both as streaming and local)

But one thing to notice is that a compressed video frame is smaller and thus less detailed than uncompressed frame. But we do live processing on (in memory) uncompressed frames. So, the live tracking algo could differ in performance from the saved version. We need a way to save uncompressed frames and make them a video.

    To do:
    [x] check that a compressed frame is different than uncompressed
    [x] how to check the size of each in 
    [x] how to save in opencv
    [ ] or do cv2.imsave() of each frame; use ffmpeg to make vid


    Notes:
    b_codec = True can allow a pop-up where you select 
    
    @680
        72  Mb 2sec uncompressed     - output12
        0.5 Mb 2sec h264-compressed  - output11
        but, no slowdown in framerate

    @1920
        400 Mb 2 sec uncompressed     - output13
          7 Mb 2 sec h264-compressed  - output14
        and yes, definetly down possibly 40% in framerate


    Get uncompressed video:
    
        https://stackoverflow.com/questions/15728939/lossless-compression-for-video-in-opencv

        https://docs.opencv.org/trunk/dd/d9e/classcv_1_1VideoWriter.html#ad59c61d8881ba2b2da22cff5487465b5
        
        http://answers.opencv.org/question/3410/saving-video-losslessly-rgb/

        This is the best answer:
        http://answers.opencv.org/question/100967/codecs-list/
            codec=0, fps=0 on vidwriter -> lossless
    
        Questions:
            [ ] does my opencv use "system ffmpeg" or only the ffmpeg.dll on the path in rootdir; and does this matter?
            [ ] how to get the vid's enconding from ffmpeg?
            [ ] Can it be watched though?

        Notes: good discussion how to use ffmpeg to make moveis from photos
        https://stackoverflow.com/questions/24961127/how-to-create-a-video-from-images-with-ffmpeg


9/12


Idea for how to track model perf:

    git-commit  algo-main       y-0     y-1     truth-y0    err-0-y0  time-perframe

    current     long-track-1    99,107  23      100,100     29.9        0.0345
    current     focus-track-2   98,107  19      100,100     9.6         0.651
    09ba        long-track-1
    876c

    y-0 is center, y-0 is radius in a particular picture
    truth-y0 is established by hand - need a contour not a blob here

    record for each git commit, this will track all changes made in other utils as well
    
